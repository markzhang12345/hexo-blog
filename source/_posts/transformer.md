---
title: "æµ…è¯» Attention is all your need"
date: 2025-10-3 11:43:47
tag: [deep learning]
categories: ç¬”è®°
toc: true
cover: /images/transformer/image-20250930182412145.png
---

# Transformer

## Abstract

ä¸»æµ Seq2Seq æ¨¡å‹ï¼ŒåŸºäº CNN å’Œ RNNï¼ŒåŒ…å« encoder å’Œ decoder çš„å¤æ‚æ¨¡å‹ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿æ¥è§£ç å™¨å’Œç¼–ç å™¨

Transformerï¼Œæ‘†è„± CNN ä¸ RNNï¼Œä»…ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶

å¥½è®­ç»ƒï¼Œç²¾åº¦é«˜ï¼Œå¯æ³›åŒ–

## Intro

å½“å‰çŠ¶æ€ä¾èµ–äºä¸Šä¸€æ¬¡è®¡ç®—å¾—åˆ°çš„çŠ¶æ€ï¼Œå› æ­¤æ¯ä¸ª token æ— æ³•è¿›è¡Œå¹¶è¡Œè®¡ç®—ï¼Œé™åˆ¶æ¨¡å‹æ€§èƒ½

Transformer ç»“æ„é¿å…å¾ªç¯ï¼Œé€šè¿‡ Self-attention è®©åºåˆ—ä¸­çš„æ¯ä¸€ä¸ª token éƒ½ä¸å…¶ä»–æ‰€æœ‰ token ç›´æ¥å»ºç«‹è”ç³»

æ¨¡å‹ç¬¬ä¸€å±‚ç›´æ¥æ¥æ”¶åˆ°çš„è¾“å…¥æ—¶åºåˆ—ä¸­æ‰€æœ‰ token åµŒå…¥å‘é‡ï¼ˆEmbedding Vectors å’Œ ä½ç½®ç¼–ç ï¼ˆPositional encoding ç»„æˆçš„çŸ©é˜µ

å¹³å‡æ³¨æ„åŠ›åŠ æƒå¯èƒ½å¯¼è‡´æœ‰æ•ˆåˆ†è¾¨ç‡é™ä½ï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡ Multi-Head æŠµæ¶ˆè¿™ç§è´Ÿé¢å½±å“

> æ¯ä¸ªç‰¹å¾å› ä¸ºæ··å…¥äº†ä¸Šä¸‹æ–‡è”ç³»ä¿¡æ¯ï¼Œå¯¼è‡´å…¶æœ¬èº«çš„ç»†èŠ‚ç‰¹å¾åœ¨ä¸€å®šç¨‹åº¦ä¸Šè¢«æ¨¡ç³Š

recurrent attention instead of sequence-aligned recurrence

å‰è€…æ˜¯ transformer é‡‡ç”¨çš„ï¼Œè®¡ç®—æ³¨æ„åŠ›çŸ©é˜µçš„æ–¹å¼ï¼Œåè€…æ˜¯ RNN ä½¿ç”¨çš„ï¼Œæ­¥è¿›è¿­ä»£å½“å‰çŠ¶æ€çš„

## Model Architecture

![image-20250930182412145](../images/transformer/image-20250930182412145.png)

### Encoder and Decoder

Encoder åŒ…å« 6 ä¸ªå®Œå…¨ç›¸åŒçš„å±‚ï¼Œæ¯å±‚æœ‰ä¸€ä¸ª Multi-Head Attention å’Œå‰é¦ˆå…¨è¿æ¥ï¼Œä¸¤ä¸ªå­å±‚éƒ½åšäº†**æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–(Add and Norm)**

> è¿™ä¸ª Add & Norm æœ‰ç‚¹ç¥ç§˜

è¿™é‡Œçš„ FFNï¼Œç±»ä¼¼é’ˆå¯¹çŸ©é˜µçš„å…¨è¿æ¥è®¡ç®—ï¼ˆè¿™é‡Œå› ä¸ºä¸Šä¸‹æ–‡ç›¸å…³ï¼Œæ‰€ä»¥ä¸ä¼šæ··åˆä½ç½®ä¿¡æ¯

Decoder åŒæ ·ç”±ä¸‰ä¸ªå­å±‚ç»„æˆï¼Œè¿™é‡Œæ¯å±‚ç¬¬ä¸€ä¸ª self-attention åŠ ä¸Šäº†æ©ç ï¼Œè¿™ä¸ªéœ€è¦ç»“åˆé¢„æµ‹å’Œè®­ç»ƒçš„ä¸åŒè¡Œä¸ºæ¥ç†è§£

- è®­ç»ƒæ—¶ï¼Œç»™ Decoder çš„è¾“å…¥æ—¶æ­£ç¡®ç­”æ¡ˆï¼Œä¸ºäº†ä¸è®©æ¨¡å‹çŸ¥é“åç»­çš„æ­£ç¡®ç­”æ¡ˆï¼ˆå› ä¸ºé¢„æµ‹æ—¶ä¸çŸ¥é“ï¼‰ï¼Œéœ€è¦æ©ç è¿›è¡Œä¿®é¥°
- è®­ç»ƒæ—¶ï¼ŒDecoder çš„è¾“å…¥æ—¶è‡ªå›å½’çš„è¾“å‡ºï¼Œæ¯ä¸€æ¬¡è¾“å‡ºçš„å†…å®¹æ—¶ä¸ä¹‹å‰çš„è¾“å‡ºæœ‰è”ç³»

### Attention

![image-20251002204744095](../images/transformer/image-20251002204744095.png)

queryï¼Œkeyï¼Œvalue ä¸‰ä¸ªå‘é‡ï¼Œæ˜ å°„æˆä¸€ä¸ª outputã€‚output æ˜¯é€šè¿‡å¯¹ value åŠ æƒæ±‚å’Œç®—å‡ºæ¥çš„ã€‚æƒé‡æ—¶é€šè¿‡ query å’Œ keyï¼Œç”¨ compatibility function ç®—å‡ºæ¥çš„

> è¿™é‡Œçš„ cpmatibility function å°±æ˜¯ Scaled Dot-Product Attention ç®—æ³•

> [!NOTE]
>
> è¿™é‡Œçš„ä¸‰ä¸ªå‘é‡æ˜¯åŸæ¥çš„è¾“å…¥ç‚¹ä¹˜äº†ç¥ç§˜å‚æ•°
>
> ![image-20250930095707639](../images/transformer/image-20250930095707639.png)

#### Scaled Dot-Product Attention

å®é™…ä¸Šå°±æ˜¯

$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

Q å’Œ K çš„è½¬ç½®ç‚¹ä¹˜ï¼Œå¾—åˆ° Q å’Œ K çš„ç›¸å…³æ€§å€¼ï¼Œé™¤ä»¥çº¬åº¦æ”¾ç¼©ï¼Œç„¶å softmax è¾“å‡ºä¸€ä¸ªæƒé‡çŸ©é˜µï¼Œæ‹¿è¿™ä¸ªçŸ©é˜µå’Œ V ç›¸ä¹˜ï¼Œå¾—åˆ° V å¾—åŠ æƒå¹³å‡

> è¿™ä¸ªåŠ æƒå¹³å‡çŸ©é˜µï¼Œæ¯ä¸€ä¸ªå‘é‡éƒ½èåˆäº†è¾“å…¥çŸ©é˜µçš„ç‰¹å¾

> [!NOTE]
>
> è®ºæ–‡ä¸­çš„æ¨¡å‹ self-attention è®¡ç®—ç›¸å…³æ€§çš„æ–¹å¼æ˜¯ Dot-productï¼Œä½†æ˜¯ä¹Ÿå¯ä»¥ä½¿ç”¨çŸ©é˜µå’Œçš„å½¢å¼ Additiveï¼Œè®ºæ–‡ä¸­æåˆ°è¿™ç§å½¢å¼å¯èƒ½åœ¨ä¸è¿›è¡Œç¼©æ”¾çš„æƒ…å†µä¸‹ä¼˜äº Dot-productï¼Œæ‰€ä»¥è¿™é‡Œä¼šä½¿ç”¨$\sqrt{d_k}$è¿›è¡Œç¼©æ”¾
>
> å®é™…ä¸Š QK ä¸¤ä¸ªå‘é‡éƒ½æ˜¯ç”¨æ¥è®¡ç®— self-attention ä¸­ç›¸å…³æ€§å¼•å‡ºçš„ï¼Œå…¶ç‚¹ç§¯ç»“æœå°±æ˜¯ Dot-product çš„ç»“æœ
>
> è¿™ä¸ªç»“æœè¡¨æ˜äº†è¾“å…¥çŸ©é˜µä¸­çš„å‘é‡ä¸¤ä¸¤ä¹‹é—´å…³ç³»çš„æƒé‡ï¼Œè¿™é‡Œå†ä¹˜ä»¥ V å‘é‡å°±ç›¸å½“äºæ±‚åŠ æƒå’Œï¼Œå¾—åˆ°çš„ç»“æœå°±æ˜¯çŸ©é˜µä¸­æŸä¸ªå‘é‡ä¸å…¶ä»–çŸ©é˜µçš„å…³ç³»åˆ†æ•°

#### Multi-Head Attention

![image-20251003125031578](../images/transformer/image-20251003125031578.png)

å°±æ˜¯é€šè¿‡ä¸åŒçš„å‚æ•°å¾—å‡ºä¸åŒç‰ˆæœ¬çš„ QKVï¼Œç„¶ååˆ†åˆ«è®¡ç®—ç›¸å…³æ€§å¾—åˆ†

æ³¨æ„æ¯ä¸ªç‰ˆæœ¬çš„ QKV å¾—åˆ°çš„éƒ½æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå°†è¿™äº›å‘é‡æ‹¼æ¥æˆä¸€ä¸ªçŸ©é˜µï¼Œç„¶åä¸å¯å­¦ä¹ å‚æ•°$W^o$ç‚¹ä¹˜ï¼Œå¾—åˆ°æœ€ç»ˆ Multi-Head Attention çš„è®¡ç®—ç»“æœ

## FFN

ä¸¤ä¸ªçº¿æ€§å…¨è¿æ¥ï¼Œä¸­é—´å¤¹ç€ä¸€ä¸ª ReLU

FFNï¼ˆå‰é¦ˆå±‚ï¼Œåˆå« MPLï¼ˆå¤šå±‚æ„ŸçŸ¥ ğŸ”

## Train

ä¼˜åŒ–å™¨ä½¿ç”¨ Adam

## Code

```python
import torch.nn as nn

# ä½¿ç”¨ Transformer base å‚æ•°
d_model = 512      # åµŒå…¥ç»´åº¦
N = 6              # ç¼–ç å™¨å’Œè§£ç å™¨çš„å±‚æ•°
h = 8              # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
d_ff = 2048        # å‰é¦ˆç¥ç»ç½‘ç»œçš„éšè—å±‚ç»´åº¦
dropout = 0.1      # Dropout æ¦‚ç‡

model = nn.Transformer(
    d_model=d_model,
    nhead=h,
    num_encoder_layers=N,
    num_decoder_layers=N,
    dim_feedforward=d_ff,
    dropout=dropout,
    batch_first=True
)

print(model)
```

```
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
```
